{"cells":[{"metadata":{},"cell_type":"markdown","source":"### **[SciBERT](http://github.com/allenai/scibert)** \n* It is a BERT model trained on scientific text.<br>\n* SciBERT is trained on papers from the corpus of semanticscholar.org. Corpus size is 1.14M papers, 3.1B tokens. We use the full text of the papers in training, not just abstracts.\n\n### Multi Class vs Multi Label Classification\n* **Multi Class** - There are multiple categories but each instance is assigned only one, therefore such problems are known as multi-class classification problem.\n* **Multi Label** - There are multiple categories and each instance can be assigned with multiple categories, so these types of problems are known as multi-label classification problem, where we have a set of target labels."},{"metadata":{"id":"vr-nvX_HT4_K"},"cell_type":"markdown","source":"# Imports"},{"metadata":{},"cell_type":"markdown","source":"The entire code is written using **PyTorch**.<br>\nWe'll be using the **transformers** library by [huggingface](https://github.com/huggingface/transformers) as they provide wrappers for multiple Transformer models."},{"metadata":{"id":"9J7Ws11-9PqG","trusted":true},"cell_type":"code","source":"# ! pip3 install transformers","execution_count":null,"outputs":[]},{"metadata":{"id":"07GPLpCt_AjQ","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport copy\nfrom tqdm.notebook import tqdm\nimport gc\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import (\n    accuracy_score, \n    f1_score, \n    classification_report\n)\n\nfrom transformers import (\n    AutoTokenizer, \n    AutoModel,\n    get_linear_schedule_with_warmup\n)\n\nproject_dir = '../input/avjanatahackresearcharticlesmlc/av_janatahack_data/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the GPU configurations. Kaggle's Tesla P100 GPU proves to be much faster for finetuning SciBERT on this dataset as compared to Google Colab's Tesla K80."},{"metadata":{"id":"-6_y9T71cQm2","trusted":true},"cell_type":"code","source":"! nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"id":"1U4JGRlCV6rU"},"cell_type":"markdown","source":"# Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://datahack-prod.s3.amazonaws.com/train_file/Train_aO7sTW8.zip\n!wget https://datahack-prod.s3.amazonaws.com/test_file/Test_H6bikL1.zip\n!unzip Train_aO7sTW8.zip\n!unzip Test_H6bikL1.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('./Train.csv')\ntest_df = pd.read_csv('./Test.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"6fTMIgN3XrPc","outputId":"7999dca4-a2cb-4d2c-bde1-2a76423be5af","trusted":true},"cell_type":"code","source":"# preprocessing\ndef clean_abstract(text):\n    text = text.split()\n    text = [x.strip() for x in text]\n    #text = [x.replace('\\n', ' ').replace('\\t', ' ') for x in text]\n    text = ' '.join(text)\n    text=re.sub(r\"(\\$+)(?:(?!\\1)[\\s\\S])*\\1\",'math_equation ',text)\n    text = re.sub('([.,!?()])', ' ', text)\n    return text\n    \n\ndef get_texts(df):\n    texts = df['ABSTRACT'].apply(clean_abstract)  \n    texts = texts.values.tolist()\n    return texts\n\n\ndef get_labels(df):\n    labels = df.iloc[:, 6:].values\n    return labels\n\ntexts = get_texts(train_df)\nlabels = get_labels(train_df)\n\nfor text, label in zip(texts[:5], labels[:5]):\n    print(f'TEXT -\\t{text}')\n    print(f'LABEL -\\t{label}')\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# no. of samples for each class\ncategories = train_df.columns.to_list()[6:]\nplt.figure(figsize=(6, 4))\n\nax = sns.barplot(categories, train_df.iloc[:, 6:].sum().values)\nplt.ylabel('Number of papers')\nplt.xlabel('Paper type ')\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no of samples having multiple labels\nrow_sums = train_df.iloc[:, 6:].sum(axis=1)\nmultilabel_counts = row_sums.value_counts()\n\nplt.figure(figsize=(6, 4))\nax = sns.barplot(multilabel_counts.index, multilabel_counts.values)\nplt.ylabel('Number of papers')\nplt.xlabel('Number of labels')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"TQ3KAWVQmDhU","trusted":true},"cell_type":"code","source":"# lengths\ny = [len(t.split()) for t in texts]\nx = range(0, len(y))\nplt.bar(x, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot above we can infer that, **320** seems like a good choice for **MAX_LENGTH**."},{"metadata":{"id":"fYncs41zT-gB"},"cell_type":"markdown","source":"# Config"},{"metadata":{},"cell_type":"markdown","source":"Here we define a Config class, which contains all the fixed parameters & hyperparameters required for **Dataset** creation as well as **Model** training."},{"metadata":{"id":"F5MG2LCST-Co","trusted":true},"cell_type":"code","source":"class Config:\n    def __init__(self):\n        super(Config, self).__init__()\n\n        self.SEED = 42\n        self.MODEL_PATH = 'allenai/scibert_scivocab_uncased'\n        self.NUM_LABELS = 25\n\n        # data\n        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_PATH)\n        self.MAX_LENGTH = 320\n        self.BATCH_SIZE = 16\n        self.VALIDATION_SPLIT = 0.2\n\n        # model\n        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.FULL_FINETUNING = True\n        self.LR = 3e-5\n        self.OPTIMIZER = 'AdamW'\n        self.CRITERION = 'BCEWithLogitsLoss'\n        self.N_VALIDATE_DUR_TRAIN = 3\n        self.N_WARMUP = 0\n        self.SAVE_BEST_ONLY = True\n        self.EPOCHS = 4\n\nconfig = Config()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset & Dataloader"},{"metadata":{},"cell_type":"markdown","source":"Now, we'll create a custom Dataset class inherited from the PyTorch Dataset class. We'll be using the **SciBERT tokenizer** that returns **input_ids** and **attention_mask**.<br>\nThe custom Dataset class will return a dict containing - <br>\n\n- input_ids\n- attention_mask\n- labels\n<br>\n\nAll three of these are inputs required by BERT models."},{"metadata":{"id":"62kL6XSQUon-","trusted":true},"cell_type":"code","source":"class TransformerDataset(Dataset):\n    def __init__(self, df, indices, set_type=None):\n        super(TransformerDataset, self).__init__()\n\n        df = df.iloc[indices]\n        self.texts = get_texts(df)\n        self.set_type = set_type\n        if self.set_type != 'test':\n            self.labels = get_labels(df)\n\n        self.tokenizer = config.TOKENIZER\n        self.max_length = config.MAX_LENGTH\n\n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, index):\n        tokenized = self.tokenizer.encode_plus(\n            self.texts[index], \n            max_length=self.max_length,\n            pad_to_max_length=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        input_ids = tokenized['input_ids'].squeeze()\n        attention_mask = tokenized['attention_mask'].squeeze()\n\n        if self.set_type != 'test':\n            return {\n                'input_ids': input_ids.long(),\n                'attention_mask': attention_mask.long(),\n                'labels': torch.Tensor(self.labels[index]).float(),\n            }\n\n        return {\n            'input_ids': input_ids.long(),\n            'attention_mask': attention_mask.long(),\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our **TransformerDataset** Class takes as input the **dataframe**, **indices** & **set_type**. We calculate the train / val set indices beforehand, pass it to **TransformerDataset** and slice the dataframe using these indices."},{"metadata":{"id":"xkGLwcZ0f732","trusted":true},"cell_type":"code","source":"# train-val split\n\nnp.random.seed(config.SEED)\n\ndataset_size = len(train_df)\nindices = list(range(dataset_size))\nsplit = int(np.floor(config.VALIDATION_SPLIT * dataset_size))\nnp.random.shuffle(indices)\n\ntrain_indices, val_indices = indices[split:], indices[:split]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we'll initialize PyTorch DataLoader objects for the training & validation data.<br>\nThese dataloaders allow us to iterate over them during training, validation or testing and return a batch of the Dataset class outputs."},{"metadata":{"id":"0mnuNOKmkc1R","outputId":"21f3404b-297b-4a07-f370-599c29f3fe38","trusted":true},"cell_type":"code","source":"train_data = TransformerDataset(train_df, train_indices)\nval_data = TransformerDataset(train_df, val_indices)\n\ntrain_dataloader = DataLoader(train_data, batch_size=config.BATCH_SIZE)\nval_dataloader = DataLoader(val_data, batch_size=config.BATCH_SIZE)\n\nb = next(iter(train_dataloader))\nfor k, v in b.items():\n    print(f'{k} shape: {v.shape}')","execution_count":null,"outputs":[]},{"metadata":{"id":"feB5OEdeoV91"},"cell_type":"markdown","source":"# Model"},{"metadata":{},"cell_type":"markdown","source":"Coming to the most interesting part - the model architecture! We'll create a class named **Model**, inherited from **torch.nn.Module**.<br><br>\n\n### Flow\n- Get **768** dimensional features from the SciBERT model.\n- Pass them through a **dropout** layer.\n- Pass the dropout layer output through a Linear layer with **input_features=768** and **output_features=25**. (25 is the number of classes)"},{"metadata":{"id":"d1xG8CCdlgge","trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n        self.transformer_model = AutoModel.from_pretrained(\n            config.MODEL_PATH\n        )\n        self.dropout = nn.Dropout(0.3)\n        self.output = nn.Linear(768, config.NUM_LABELS)\n\n    def forward(\n        self,\n        input_ids, \n        attention_mask=None, \n        token_type_ids=None\n        ):\n\n        _, o2 = self.transformer_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        x = self.dropout(o2)\n        x = self.output(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"id":"jiypz6jrG9bl","trusted":true},"cell_type":"code","source":"device = config.DEVICE\ndevice","execution_count":null,"outputs":[]},{"metadata":{"id":"O5LkYPeTqpDP"},"cell_type":"markdown","source":"# Engine"},{"metadata":{},"cell_type":"markdown","source":"Our engine consists of the training and validation step functions."},{"metadata":{"id":"ZJtj7C0pqMIC","trusted":true},"cell_type":"code","source":"def val(model, val_dataloader, criterion):\n    \n    val_loss = 0\n    true, pred = [], []\n    \n    # set model.eval() every time during evaluation\n    model.eval()\n    \n    for step, batch in enumerate(val_dataloader):\n        # unpack the batch contents and push them to the device (cuda or cpu).\n        b_input_ids = batch['input_ids'].to(device)\n        b_attention_mask = batch['attention_mask'].to(device)\n        b_labels = batch['labels'].to(device)\n\n        # using torch.no_grad() during validation/inference is faster -\n        # - since it does not update gradients.\n        with torch.no_grad():\n            # forward pass\n            logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n            \n            # calculate loss\n            loss = criterion(logits, b_labels)\n            val_loss += loss.item()\n\n            # since we're using BCEWithLogitsLoss, to get the predictions -\n            # - sigmoid has to be applied on the logits first\n            logits = torch.sigmoid(logits)\n            logits = np.round(logits.cpu().numpy())\n            labels = b_labels.cpu().numpy()\n            \n            # the tensors are detached from the gpu and put back on -\n            # - the cpu, and then converted to numpy in order to -\n            # - use sklearn's metrics.\n\n            pred.extend(logits)\n            true.extend(labels)\n\n    avg_val_loss = val_loss / len(val_dataloader)\n    print('Val loss:', avg_val_loss)\n    print('Val accuracy:', accuracy_score(true, pred))\n\n    val_micro_f1_score = f1_score(true, pred, average='micro')\n    print('Val micro f1 score:', val_micro_f1_score)\n    return val_micro_f1_score\n\n\ndef train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch):\n    \n    # we validate config.N_VALIDATE_DUR_TRAIN times during the training loop\n    nv = config.N_VALIDATE_DUR_TRAIN\n    temp = len(train_dataloader) // nv\n    temp = temp - (temp % 100)\n    validate_at_steps = [temp * x for x in range(1, nv + 1)]\n    \n    train_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader, \n                                      desc='Epoch ' + str(epoch))):\n        # set model.eval() every time during training\n        model.train()\n        \n        # unpack the batch contents and push them to the device (cuda or cpu).\n        b_input_ids = batch['input_ids'].to(device)\n        b_attention_mask = batch['attention_mask'].to(device)\n        b_labels = batch['labels'].to(device)\n\n        # clear accumulated gradients\n        optimizer.zero_grad()\n\n        # forward pass\n        logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n        \n        # calculate loss\n        loss = criterion(logits, b_labels)\n        train_loss += loss.item()\n\n        # backward pass\n        loss.backward()\n\n        # update weights\n        optimizer.step()\n        \n        # update scheduler\n        scheduler.step()\n\n        if step in validate_at_steps:\n            print(f'-- Step: {step}')\n            _ = val(model, val_dataloader, criterion)\n    \n    avg_train_loss = train_loss / len(train_dataloader)\n    print('Training loss:', avg_train_loss)","execution_count":null,"outputs":[]},{"metadata":{"id":"ApdIReKk7OQJ"},"cell_type":"markdown","source":"# Run"},{"metadata":{},"cell_type":"markdown","source":"### Loss function used<br>\n- **BCEWithLogitsLoss** - Most commonly used loss function for Multi Label Classification tasks. Note that, PyTorch's BCEWithLogitsLoss is numerically stable than BCELoss.\n<br>\n\n### Optimizer used <br>\n- **AdamW** - Commonly used optimizer. Performs better than Adam.\n<br>\n\n### Scheduler used <br>\n- **get_linear_scheduler_with_warmup** from the **transformers** library.\n<br>"},{"metadata":{"id":"93Qs1xH27FRe","trusted":true},"cell_type":"code","source":"def run():\n    # setting a seed ensures reproducible results.\n    # seed may affect the performance too.\n    torch.manual_seed(config.SEED)\n\n    criterion = nn.BCEWithLogitsLoss()\n    \n    # define the parameters to be optmized -\n    # - and add regularization\n    if config.FULL_FINETUNING:\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.001,\n            },\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        optimizer = optim.AdamW(optimizer_parameters, lr=config.LR)\n\n    num_training_steps = len(train_dataloader) * config.EPOCHS\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps\n    )\n\n    max_val_micro_f1_score = float('-inf')\n    for epoch in range(config.EPOCHS):\n        train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch)\n        val_micro_f1_score = val(model, val_dataloader, criterion)\n\n        if config.SAVE_BEST_ONLY:\n            if val_micro_f1_score > max_val_micro_f1_score:\n                best_model = copy.deepcopy(model)\n                best_val_micro_f1_score = val_micro_f1_score\n\n                model_name = 'scibertfft_best_model'\n                torch.save(best_model.state_dict(), model_name + '.pt')\n\n                print(f'--- Best Model. Val loss: {max_val_micro_f1_score} -> {val_micro_f1_score}')\n                max_val_micro_f1_score = val_micro_f1_score\n\n    return best_model, best_val_micro_f1_score","execution_count":null,"outputs":[]},{"metadata":{"id":"C5BwV_228Cok","outputId":"da336ea0-9009-4cbb-d23f-c92e444e327b","trusted":true},"cell_type":"code","source":"model = Model()\nmodel.to(device);","execution_count":null,"outputs":[]},{"metadata":{"id":"YMwlc6aG7e4I","outputId":"bdd17bf0-b090-4fbb-a875-9071bc00ef5f","trusted":true},"cell_type":"code","source":"best_model, best_val_micro_f1_score = run()","execution_count":null,"outputs":[]},{"metadata":{"id":"4nfT_foB9-yr"},"cell_type":"markdown","source":"# Submission"},{"metadata":{},"cell_type":"markdown","source":"Load the test dataset, and initialize and DataLoader object for it."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"./Test.csv\")\ndataset_size = len(test_df)\ntest_indices = list(range(dataset_size))\n\ntest_data = TransformerDataset(test_df, test_indices, set_type='test')\ntest_dataloader = DataLoader(test_data, batch_size=config.BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"id":"JI-MUk0i-FHH","trusted":true},"cell_type":"code","source":"def predict(model):\n    val_loss = 0\n    test_pred = []\n    model.eval()\n    for step, batch in enumerate(test_dataloader):\n        b_input_ids = batch['input_ids'].to(device)\n        b_attention_mask = batch['attention_mask'].to(device)\n\n        with torch.no_grad():\n            logits = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n            logits = torch.sigmoid(logits)\n            logits = np.round(logits.cpu().numpy())\n            test_pred.extend(logits)\n\n    test_pred = np.array(test_pred)\n    return test_pred\n\ntest_pred = predict(best_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://datahack-prod.s3.amazonaws.com/sample_submission/SampleSubmission_Uqu2HVA.csv","execution_count":null,"outputs":[]},{"metadata":{"id":"nK_7216XA2Ah","trusted":true},"cell_type":"code","source":"TARGET_COLS = ['Analysis of PDEs', 'Applications',\n               'Artificial Intelligence', 'Astrophysics of Galaxies',\n               'Computation and Language', 'Computer Vision and Pattern Recognition',\n               'Cosmology and Nongalactic Astrophysics',\n               'Data Structures and Algorithms', 'Differential Geometry',\n               'Earth and Planetary Astrophysics', 'Fluid Dynamics',\n               'Information Theory', 'Instrumentation and Methods for Astrophysics',\n               'Machine Learning', 'Materials Science', 'Methodology', 'Number Theory',\n               'Optimization and Control', 'Representation Theory', 'Robotics',\n               'Social and Information Networks', 'Statistics Theory',\n               'Strongly Correlated Electrons', 'Superconductivity',\n               'Systems and Control']\nsample_submission = pd.read_csv(\"./SampleSubmission_Uqu2HVA.csv\")\nsample_submission[TARGET_COLS]=test_pred\nsample_submission","execution_count":null,"outputs":[]},{"metadata":{"id":"bS_suOKxERTd","trusted":true},"cell_type":"code","source":"#submission_fname = f'submission_scibertfft_microf1-{round(best_val_micro_f1_score, 4)}.csv'\nsample_submission.to_csv(\"bert 74.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}